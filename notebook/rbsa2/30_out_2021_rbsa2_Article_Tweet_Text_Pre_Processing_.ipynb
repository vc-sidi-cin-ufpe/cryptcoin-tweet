{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "30-out-2021_rbsa2_Article-Tweet-Text-Pre-Processing .ipynb",
      "provenance": [],
      "collapsed_sections": [
        "zYULmLrhoWap",
        "VPZTPmOooS3M",
        "TxM2r213MMQk",
        "vdicRyNZMMQk",
        "dZFtbxIHohYP",
        "MC2oBNtWokPF"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYIoIT7lco1y"
      },
      "source": [
        "# Install Dependecies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vfMWRFYljJk"
      },
      "source": [
        "#### Lime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDpdvjpFzZrC",
        "outputId": "86d17b92-afcc-4dae-d3a5-261613d3cda1"
      },
      "source": [
        "!pip install spacy\n",
        "!spacy download en"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.8.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 10.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.5.30)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ku0MtBbclmg1"
      },
      "source": [
        "#### Langid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQU97VnPhIjg",
        "outputId": "1c19d923-5a57-4bc6-f690-ddb68a01aab3"
      },
      "source": [
        "!pip install langid"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langid\n",
            "  Downloading langid-1.1.6.tar.gz (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 12.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from langid) (1.19.5)\n",
            "Building wheels for collected packages: langid\n",
            "  Building wheel for langid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langid: filename=langid-1.1.6-py3-none-any.whl size=1941187 sha256=09a0d4b6194c119ec63c48215c21dab41d954624f8326d4112a5f4f4925d96e0\n",
            "  Stored in directory: /root/.cache/pip/wheels/2b/bb/7f/11e4db39477278161e882eadc46fb558949a28b13470fc74b8\n",
            "Successfully built langid\n",
            "Installing collected packages: langid\n",
            "Successfully installed langid-1.1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6TUf6qacML8"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqsOCgxvcDes"
      },
      "source": [
        "import spacy\n",
        "## for data\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "## for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "## for processing\n",
        "import nltk\n",
        "## for bag-of-words"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIhMrKQVieqY"
      },
      "source": [
        "#regex\n",
        "import re"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9srJm90JlYVE"
      },
      "source": [
        "## Download Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sfl6yCeslali",
        "outputId": "85065e1a-e31f-4987-e355-2eceacf75957"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('rslp')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping stemmers/rslp.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmVZtr-MaCMk"
      },
      "source": [
        "#### Carrega spacy corpus em ingles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svvehGLwaBid",
        "outputId": "ed085946-e0bf-4a29-e3cc-268c52613c9b"
      },
      "source": [
        "%%time\n",
        "spacy_nlp = spacy.load('en')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 668 ms, sys: 87.5 ms, total: 756 ms\n",
            "Wall time: 956 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vy6s3xd1gSVk"
      },
      "source": [
        "## Preprocess Text Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXfw8JslaTGk"
      },
      "source": [
        "### Lemmatize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBSuCuAf0M7P"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "def lemmatize_spacy(text):\n",
        "  doc = spacy_nlp(text)\n",
        "  lemmatize_words = []\n",
        "  lemmatize_words += [token.lemma_ for token in doc]\n",
        "  return lemmatize_words"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TzW7cAT3qJ7"
      },
      "source": [
        "### Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSXX6o9v3phr",
        "outputId": "d768507e-ae09-4160-9714-e32120eb4d02"
      },
      "source": [
        "%%time\n",
        "lst_stopwords = nltk.corpus.stopwords.words(\"english\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.61 ms, sys: 1.15 ms, total: 2.77 ms\n",
            "Wall time: 4.3 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbgKWJ61e-k0"
      },
      "source": [
        "### Remove Retweet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yz-Vof2we_vm"
      },
      "source": [
        "def remove_retweet(text):\n",
        "  return re.sub(r'rt+','',text )"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBZ2w8uniEfr"
      },
      "source": [
        "### Reduce Char Sequence  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yi2ekJKOiD-t"
      },
      "source": [
        "#http://homepages.math.uic.edu/~leon/mcs425-s08/handouts/doublechar_freq.pdf\n",
        "def reduce_caracter_sequece(text):\n",
        "  text = re.sub(r'j{2,}','j',text )\n",
        "  text = re.sub(r'k{2,}','k',text )\n",
        "  text = re.sub(r'q{2,}','q',text )\n",
        "  text = re.sub(r'v{2,}','v',text )\n",
        "  text = re.sub(r'x{2,}','x',text )\n",
        "  text =re.sub(r'z{2,}','z',text )\n",
        "  text =re.sub(r'u{2,}','u', text)\n",
        "\n",
        "  text= re.sub(r'a{3,}','aa',text)\n",
        "  text = re.sub(r'b{3,}','bb',text )\n",
        "  text = re.sub(r'c{3,}','cc',text )\n",
        "  text =re.sub(r'd{3,}','dd',text )\n",
        "  text = re.sub(r'e{3,}','ee',text )\n",
        "  text =re.sub(r'f{3,}','ff',text )\n",
        "  text = re.sub(r'g{3,}','gg', text)\n",
        "  text = re.sub(r'h{3,}','hh', text)\n",
        "  text = re.sub(r'i{3,}','ii', text)\n",
        "  text = re.sub(r'l{3,}','ll', text)\n",
        "  text = re.sub(r'm{3,}','mm', text)\n",
        "  text = re.sub(r'l{3,}','nn', text)\n",
        "  text = re.sub(r'p{3,}','pp', text)\n",
        "  text = re.sub(r'r{3,}','rr', text)\n",
        "  text = re.sub(r's{3,}','ss', text)\n",
        "  text = re.sub(r't{3,}','tt', text)\n",
        "  text = re.sub(r'w{3,}','ww', text)\n",
        "  text =re.sub(r'y{3,}','yy', text)\n",
        "\n",
        "  return text"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4uwtpk9zs3w"
      },
      "source": [
        "### Remove Digit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Glfuwx0PuWO3"
      },
      "source": [
        "def remove_digit(text):\n",
        "  return re.sub(r'\\d','', text )"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOjMsRHTzuoh"
      },
      "source": [
        "### Handle Slash and Hastag"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilUNV5tizx4U"
      },
      "source": [
        "def remove_slash_hashtag(text):\n",
        "  text = re.sub(r'#\\w{0,3} ','',text )\n",
        "  text = re.sub(r'\\$\\w{0,3}','',text )\n",
        "  text = re.sub(r'\\@\\w{0,}','',text )\n",
        "  return text"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBSqiPQ80iTc"
      },
      "source": [
        "### Remove Hyperlink"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaZQF4qv0mVN"
      },
      "source": [
        "def remove_hyperlink(text):\n",
        "  return re.sub(r'http\\S+','', text)\n"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0URCrdVPbNm2"
      },
      "source": [
        "### Preprocess Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rhe-qGrGgSk3"
      },
      "source": [
        "'''\n",
        "Preprocess a string.\n",
        ":parameter\n",
        "    :param text: string - name of column containing text\n",
        "    :param lst_stopwords: list - list of stopwords to remove\n",
        "    :param flg_stemm: bool - whether stemming is to be applied\n",
        "    :param flg_lemm: bool - whether lemmitisation is to be applied\n",
        ":return\n",
        "    cleaned text\n",
        "'''\n",
        "def lemmatize_spacy(text):\n",
        "  doc = spacy_nlp(text)\n",
        "  lemmatize_words = []\n",
        "  lemmatize_words += [token.lemma_ for token in doc]\n",
        "  return lemmatize_words\n",
        "\n",
        "def remove_stopwords(lst_text):\n",
        "  if lst_stopwords is not None:\n",
        "        lst_text = [word for word in lst_text if word not in \n",
        "                    lst_stopwords]\n",
        "  return lst_text\n",
        "\n",
        "def utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n",
        "    ## clean (convert to lowercase and remove punctuations and  characters and then strip)\n",
        "\n",
        "    ## lower and stript\n",
        "    text = str(text).lower().strip()\n",
        "\n",
        "    ## remove remove_slash_hashtag #,$,@\n",
        "    text = remove_slash_hashtag(text)\n",
        "\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    ## remove 'rt'\n",
        "    text = remove_retweet(text)\n",
        "\n",
        "    # remove hyperlink\n",
        "    text = remove_hyperlink(text)\n",
        "\n",
        "    ## reduce_caracter_sequece uuu, ttt, zzz\n",
        "    text = reduce_caracter_sequece(text)\n",
        "\n",
        "    ## remove digit 0, 9\n",
        "    text = remove_digit(text)\n",
        "\n",
        "   \n",
        "\n",
        "    #remove             \n",
        "    ## Tokenize (convert from string to list)\n",
        "    lst_text = text.split()\n",
        "    ## remove Stopwords\n",
        "    if lst_stopwords is not None:\n",
        "        lst_text = [word for word in lst_text if word not in \n",
        "                    lst_stopwords]\n",
        "                \n",
        "    ## Stemming (remove -ing, -ly, ...)\n",
        "    if flg_stemm == True:\n",
        "        ps = nltk.stem.RSLPStemmer()\n",
        "        lst_text = [ps.stem(word) for word in lst_text]\n",
        "                \n",
        "    ## Lemmatisation (convert the word into root word)\n",
        "    if flg_lemm == True:\n",
        "      #print(lst_text)\n",
        "      lst_text =  lemmatize_spacy(\" \".join(lst_text))\n",
        "      \"\"\"lemma_words = []\n",
        "      for sent in nlp(\" \".join(lst_text)).sentences:\n",
        "        for word in sent.words:\n",
        "          lemma_words.append(word.lemma)\n",
        "        #lst_text =  [ word.lemma  for word in sent.words ]\n",
        "      lst_text = lemma_words\"\"\"\n",
        "      \"\"\"lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "      lst_text = [lem.lemmatize(word) for word in lst_text]\"\"\"\n",
        "          \n",
        "    ## back to string from list\n",
        "    text = \" \".join(lst_text)\n",
        "    return text"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQdB0NDDbVau"
      },
      "source": [
        "## Teste Tweet String Samples from article"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq2WC15Cc7Ro"
      },
      "source": [
        "### Expected Text "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YEPjGr-wc-pv",
        "outputId": "17907227-eaa0-4941-cc39-06132185d48d"
      },
      "source": [
        "expected_text = \"bitcoin etf rejected but buuuy ask not buying laughing out loud tomorrow reach buy\"\n",
        "expected_text"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'bitcoin etf rejected but buuuy ask not buying laughing out loud tomorrow reach buy'"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkrZFPjrdQ4C"
      },
      "source": [
        "### Length Expected tweet string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cz1TO5BsdOPS",
        "outputId": "a144a3b4-decc-468a-cf18-969ae3bb3d32"
      },
      "source": [
        "len(expected_text)"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "82"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TawjBxl-baf1"
      },
      "source": [
        "### Set Raw Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4Zc2J9vbdet"
      },
      "source": [
        "text = \"RT @bitcoin https://twitter.com/FT/status/1022605086172872704 Bitcoin ETF rejected but buuuuuy!!!                           Ask yourself why you aren't buying lol, tomorrow it will reach 8000 #BUY #NOW #BITCOIN #BTC $BTC $ETH\""
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1PsKmG5MbZRf",
        "outputId": "59922d5e-9729-4332-df2e-36c61b29ab5a"
      },
      "source": [
        "preprocessing_text = utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=lst_stopwords)\n",
        "preprocessing_text"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'bitcoin etf reject buy ask be not buy lol tomorrow reach bitcoin'"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXb5dCPEdpR2"
      },
      "source": [
        "### Length Preprocesing tweet string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuA4L72Pde4p",
        "outputId": "1aa26ace-5474-4292-f8b1-bee6e2985079"
      },
      "source": [
        "len(preprocessing_text)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZEV4EsodzLR"
      },
      "source": [
        "### Compare (Expected x Preprocessing) Tweet Strings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_tQQeH15d4Ng",
        "outputId": "3496eee9-c570-4886-80c3-55dee5d6148b"
      },
      "source": [
        "expected_text"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'bitcoin etf rejected but buuuy ask not buying laughing out loud tomorrow reach buy'"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QVlEgJ5_d13a",
        "outputId": "23b2e466-f095-4cfb-ac1a-dd9d8d08c95d"
      },
      "source": [
        "preprocessing_text"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'bitcoin etf reject buy ask be not buy lol tomorrow reach bitcoin'"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wb4jCqjfkfvW",
        "outputId": "c53c8d08-8d1c-4281-96c0-4db4de49993c"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "m_ctGIahvMsP",
        "outputId": "0df3f490-42e2-4af9-f242-4e46e8b0b474"
      },
      "source": [
        "text"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"RT @bitcoin https://twitter.com/FT/status/1022605086172872704 Bitcoin ETF rejected but buuuuuy!!!                           Ask yourself why you aren't buying lol, tomorrow it will reach 8000 #BUY #NOW #BITCOIN #BTC $BTC $ETH\""
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptsVD6mEbGQf"
      },
      "source": [
        "## Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bud3gzfMfCGW"
      },
      "source": [
        "import random\n",
        "\n",
        "p = 0.01"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtY0py6-fYdK"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsxI6H4dy2qQ"
      },
      "source": [
        "%%time\n",
        "dtf = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Dataset/tweets.csv\", skiprows=lambda i: i>0 and random.random() > p, error_bad_lines=False, nrows=250000, sep=';', lineterminator='\\n')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yw0ja6KqReKs"
      },
      "source": [
        "dtf.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHKT6CM1l8XW"
      },
      "source": [
        "## Checking Tweets Language"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QarIjsHBg83U"
      },
      "source": [
        "%%time\n",
        "#vê um melhor\n",
        "import langid\n",
        "#langid.classify(\"Soy muy\")\n",
        "dtf['language'] = dtf['text\\r'].apply(lambda x: langid.classify(x)[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVEDva81nNI_"
      },
      "source": [
        "## Filter English Language"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biuUh50YhZZ5"
      },
      "source": [
        "#filtrar para EN\n",
        "dtf = dtf[dtf['language']=='en']\n",
        "dtf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrX9E6sh1y81"
      },
      "source": [
        "dtf.to_csv('/content/drive/MyDrive/Colab Notebooks/Dataset/language_en_tweets_dataset.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQ8lbHnjLvfL"
      },
      "source": [
        "### Stopwords Lemmatisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1TC_azJgmE5"
      },
      "source": [
        "%%time\n",
        "dtf[\"text_clean\"] = dtf[\"text\\r\"].apply(lambda x: utils_preprocess_text(x, flg_stemm=False, flg_lemm=True, lst_stopwords=lst_stopwords))\n",
        "samples = dtf.sample(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYULmLrhoWap"
      },
      "source": [
        "#### Check Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vU_7RtV-8uQ"
      },
      "source": [
        "print(remove_stopwords(samples[\"text\\r\"]))\n",
        "print(list(samples[\"text_clean\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPZTPmOooS3M"
      },
      "source": [
        "#### Export to csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnt_s2sE_X42"
      },
      "source": [
        "dtf.to_csv('/content/drive/MyDrive/Colab Notebooks/Dataset/stopwords_lemmatisation_tweets_preprocessing_dataset.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ti8b2TPlMMQj"
      },
      "source": [
        "### Stopwords Steeming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yv2Ld3DNMMQj"
      },
      "source": [
        "%%time\n",
        "dtf[\"text_clean\"] = dtf[\"text\\r\"].apply(lambda x: utils_preprocess_text(x, flg_stemm=True, flg_lemm=False, lst_stopwords=lst_stopwords))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxM2r213MMQk"
      },
      "source": [
        "#### Check Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0DL24v9MMQk"
      },
      "source": [
        "samples = dtf.sample(199)\n",
        "print(remove_stopwords(samples[\"text\\r\"]))\n",
        "print(list(samples[\"text_clean\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdicRyNZMMQk"
      },
      "source": [
        "#### Export to csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbWMoUWmMMQk"
      },
      "source": [
        "dtf.to_csv('/content/drive/MyDrive/Colab Notebooks/Dataset/stopwords_stemming_tweets_preprocessing_dataset.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laXMMreVoZmn"
      },
      "source": [
        "### Pre Processing, Stemming Lemm and Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lg4VHW73AOrr"
      },
      "source": [
        "%%time\n",
        "dtf[\"text_clean\"] = dtf[\"text\\r\"].apply(lambda x: utils_preprocess_text(x, flg_stemm=True, flg_lemm=True, lst_stopwords=lst_stopwords))\n",
        "samples = dtf.sample(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXhFsqv-OObG"
      },
      "source": [
        "dtf.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZFtbxIHohYP"
      },
      "source": [
        "#### Check Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxwdTCpT29k0"
      },
      "source": [
        "samples = dtf.sample(599)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1LrGbvOAP1X"
      },
      "source": [
        "print(remove_stopwords(samples[\"text\\r\"]))\n",
        "print(list(samples[\"text_clean\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC2oBNtWokPF"
      },
      "source": [
        "#### Export to CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ri_p38sUAS66"
      },
      "source": [
        "dtf.to_csv('/content/drive/MyDrive/Colab Notebooks/Dataset/stopwords_stemming_lemmatisation_tweets_preprocessing_dataset.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uusivlcfTav0"
      },
      "source": [
        "dtf"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}